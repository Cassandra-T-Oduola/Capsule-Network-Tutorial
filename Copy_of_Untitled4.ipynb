{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cassandra-T-Oduola/Capsule-Network-Tutorial/blob/master/Copy_of_Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "T7qpI94_ElOn",
        "colab_type": "code",
        "outputId": "75272233-7c17-4e2f-e1ca-eb28730efb6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/taoxugit/AttnGAN.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AttnGAN'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 36.76 MiB | 9.03 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nZHjiLCfRT9P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "93e62c48-69e3-493a-b5e9-c8132c4ee1d6"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install unzip"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M-zD9gv-MPVF",
        "colab_type": "code",
        "outputId": "33869116-29c9-4dde-f145-19bd3f59d6fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "annotation_zip = tf.keras.utils.get_file('captions.zip', \n",
        "                                          cache_subdir=os.path.abspath('.'),\n",
        "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                          extract = True)\n",
        "annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "\n",
        "name_of_zip = 'train2014.zip'\n",
        "if not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\n",
        "  image_zip = tf.keras.utils.get_file(name_of_zip, \n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract = True)\n",
        "  PATH = os.path.dirname(image_zip)+'/train2014/'\n",
        "else:\n",
        "  PATH = os.path.abspath('.')+'/train2014/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "252878848/252872794 [==============================] - 21s 0us/step\n",
            "252887040/252872794 [==============================] - 21s 0us/step\n",
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510574080/13510573713 [==============================] - 807s 0us/step\n",
            "13510582272/13510573713 [==============================] - 807s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8PQj3I7XFBjV",
        "colab_type": "code",
        "outputId": "c82207e4-d50f-4815-8607-180b768df430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install easydict"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: easydict in /usr/local/lib/python2.7/dist-packages (1.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aRnQbg8wFHPm",
        "colab_type": "code",
        "outputId": "d5ff7259-b425-48e0-d4fa-d37949412d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (0.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ekk8JlLGFYSu",
        "colab_type": "code",
        "outputId": "591c3cc8-7cfe-4a3d-9557-dca6a568f04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (from torchvision) (0.4.1)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/f6/3b3c82c5c75cae471e02fb584136168d732e17ae9db2d21c5dc82f9790f8/Pillow-5.3.0-cp27-cp27mu-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 10.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xS3EN-trIAsc",
        "colab_type": "code",
        "outputId": "0a8ee183-a9a1-43b9-9c7b-209d1d19a81b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os.chdir('/content/AttnGAN/')\n",
        "\n",
        "!python code/pretrain_DAMSM.py --cfg code/cfg/DAMSM/coco.yml --gpu 0"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'DAMSM',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'coco',\n",
            " 'DATA_DIR': '../data/coco',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 128,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 5, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 15},\n",
            " 'TRAIN': {'BATCH_SIZE': 48,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '',\n",
            "           'NET_G': '',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 1.0},\n",
            "           'SNAPSHOT_INTERVAL': 5},\n",
            " 'TREE': {'BASE_SIZE': 299, 'BRANCH_NUM': 1},\n",
            " 'WORKERS': 1}\n",
            "/usr/local/lib/python2.7/dist-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Traceback (most recent call last):\n",
            "  File \"code/pretrain_DAMSM.py\", line 243, in <module>\n",
            "    transform=image_transform)\n",
            "  File \"/content/AttnGAN/code/datasets.py\", line 117, in __init__\n",
            "    self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
            "  File \"/content/AttnGAN/code/datasets.py\", line 230, in load_text_data\n",
            "    with open(filepath, 'wb') as f:\n",
            "IOError: [Errno 2] No such file or directory: u'../data/coco/captions.pickle'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pIBNVXQNE0Xw",
        "colab_type": "code",
        "outputId": "44938f4c-e116-44d8-8c4e-66a8ed8da551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "cell_type": "code",
      "source": [
        "!python AttnGAN/code/main.py --cfg AttnGAN/code/cfg/coco_attn2.yml --gpu 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'glu-gan2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'coco',\n",
            " 'DATA_DIR': '../data/coco',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 96,\n",
            "         'GF_DIM': 48,\n",
            "         'R_NUM': 3,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 3,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 5, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 12},\n",
            " 'TRAIN': {'BATCH_SIZE': 14,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 120,\n",
            "           'NET_E': '../DAMSMencoders/coco/text_encoder100.pth',\n",
            "           'NET_G': '',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 50.0},\n",
            "           'SNAPSHOT_INTERVAL': 5},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "/usr/local/lib/python2.7/dist-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Traceback (most recent call last):\n",
            "  File \"AttnGAN/code/main.py\", line 129, in <module>\n",
            "    transform=image_transform)\n",
            "  File \"/content/AttnGAN/code/datasets.py\", line 116, in __init__\n",
            "    self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n",
            "  File \"/content/AttnGAN/code/datasets.py\", line 229, in load_text_data\n",
            "    with open(filepath, 'wb') as f:\n",
            "IOError: [Errno 2] No such file or directory: u'../data/coco/captions.pickle'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "paHqBJBIH49C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}